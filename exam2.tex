\documentclass[letterpaper,onecolumn,titlepage]{Ythesis}
\usepackage{siunitx}
\usepackage{amsmath}
\usepackage{url}
\sisetup{detect-weight=true, detect-family=true}
\newcommand{\micron}[1] {\SI{#1}{\micro\meter}}

%\usepackage{setspace}
%\onehalfspace

\title{Contextualizing Climate Ensembles}
\author{Hannah Aizenman}
\committee{ Dr. Michael Grossberg(Advisor), Robert Haralick, Huy Vo}
\submitted{}
\abstract{
  Problem: Ensembles inherently encode uncertainity since they are different
  variations on the same model. While there are many ways to visualize
  ensembles, and the uncertianity in the ensembles, these visualizations are
  often at least a step or two removed from what is happening in the real
  world. 
  Importance: Climate & weather ensembles are widley used by scientists and
  policy makers to evaluate risk; this is readily seen in Hurrican season when
  governor's have to decide on whether to move people based on how sure they
  are that the model's predictions are really accurate. Decontextualized
  visualizations that are more about the method than the data can occlude this
  decision process. 
  
  Solution: THere are a kitchen sinks worth of methods and toolkits for
  visualizing ensembles, but the best ones clearly articulate the
  spatio-temporal relationships in the ensembles and between the variables such
  that the user can definitively parse out what a line or dot is saying about
  the temperature in New Jersey.  
}

\begin{document}
\makefrontmatter

\section{Introduction}

``Contextualize all the things'' is important. Tufte stresses it because he's
confirmatory, but Munzner's exploratray task-data-idiom approach is also
grounded in it. This is because while very pretty visualizations look well
pretty, at the end of the day they're semi-useless if there are too many steps
removed between the visualization and the underlying data. 

%Large datasets of climate and weather data are being produced from multiple sources. With cheaper and increased computer power, and cheaper disk storage, it is now possible to more easily create long model simulations at higher resolutions. Increasingly  high 
%resolution satellite data and reanalysis projects which blend both model prediction and measurements create readily available large uniformly sampled spatio-temporal resolution datasets. Measured data from networks of ground stations and proxy data from trees, 
%glaciers, oceans, corals, fossils, and historical records are also becoming more accessible, 
%yielding sometimes irregularly sampled data that provides a host of 
%information on climate variability. The great size of many of these data sets mean that just providing a download link does not really make the data accessible to the public or even other scientists. It is important for the scientists who produce the data to have a way to make it explorable.. 

And that those explorations are understandable...while many papers(\cite{Munzner} \cite{Acquired Codes} \cite{...}discuss how more complex tasks inherently encourage more complex visualizations, they also warn that more complex visual idioms are often more difficult to understand and so they may not be beneficial. Some authors though discuss how highly specialized dashboards may be more productive once specialists are trained on the software.

Organization:
\begin{table}
data & task & idiom  & AUDIENCE\\

\end{table} 



\subsection{Scope}

This paper focuses on the visualization of:
\begin{itemize}
\item climate and weather data - observational and model
\item has both a spaitial and temporal component
\item requires aggregation(ML or stats) before can be visualized with simple viz (line, bar, pie, scatter, heatmap)
\end{itemize}



%%some sorta transition from the generic to the specific tools-and shift this to be more tool oriented


Munzner \cite{Munzner14} defines a time-varying dataset as one in which time is an intrinsic attribute of how the various observations are measured; for example recording snow fall every 3 hours or stock prices at the beginning or end of day. She contrasts this with datasets that are amassed over time where the length of the record does not intrinsically mean the data is time varying; for example [insert something that isn't horse racing] 

Often the observations in these time-varying datasets are also spatially varying, spreading over the earth or the brain or another variable space where all the observations in the high dimensional space are of the same unit. %%note: must get some reference/notation/way to discuss this

This poses a difficult visualization task because the researcher is trying to capture multiple levels of interaction:
\begin{enumerate}
	\item intraobservational: snowfall in New York and snowfall in New Jersey on August 9th 2015)
	\item interobservational: global snowfall on August 9th and August 10th
	\item a mixture thereof: does snowfall in New York on the 9th affect snowfall in New Jersey on the 10th?
\end{enumerate}


\section{Simple idioms, hard math}

\subsection{Probabilistic aggregations/information gain metrics}
\label{sec:probabilistic}
%%Dump Nir into here-it kinda ties into the band depth stuff that'll show up later
\subsection{markov chain analysis}
\subsection{clustering}

\section{Let's throw Everything in one picture}                                                        
%% Evolution of Single Task/Single Idiom to Higher Order 
\subsection{Scatter, matrix of scatter, scagnostics}                                        
\subsection{Quiver, Flow, Spaghetti, Lasagna}
\subsection{BoxPlots, Functional Box Plots, Countour Box Plots}                                      
A box plot is a really simple measure of uncertainity: there's a line and then two bars represent the uncertainity of that point. %insert image
While this is fairly readable for small datasets, %%\cite{} argues that the upper limit is x/y/z. 




Countour Boxplots \cite{Whitaker2013} are an extension of boxplots that tries to better encapsulate outliers. Box plots inherently clip the uncertainity they show to some upper and lower band, 
creating a somewhat bounding envelope for the function that looks fairly regular when aggregated. Countour boxplots aim instead to capture the variablity of the uncertainity by trading in descripting
statistics for a measure called band depth. Each ensemble members band depth is computed as sum of the probabilities that the observations in any given ensemble fall within the max-min envelope defined by any two other ensembles. The bands are then sorted by band depth such that the median is the ensemble with about 50\% of its members withen all envelopes formed by other bands (so most centered). Outer bands are chosen according to the task at hand. In \cite{Whitaker2013}, they apply the countour boxplots to temperature visualizations. %%insert figures here. 
The authors argue that countour boxplots are an improved visual idiom over the traditional spagehttie plots seen in (fig) because as seen in (fig), the spagehetti plots get nosy as the number of plots increase and so it's hard to tease out specific patterns. Instead in (fig3), the authors remove most of the bands and instead visualize an outliser envelope(light gray) and a more central envelope(dark gray). It retains much of the information of the spagethhit, but removes the visual noise of the lines.  
\subsection{Maps & Heatmaps -> Glyphs}     


\subsection{Let's Jam all the simple together as Toolkits}
Some research institutions provide a tool to explore data they created, such as the \cite{src:esrlpsd}, but since all these tools are site specific, a researcher would either have to use the datasets on the website or submit their datasets for uploading 
(assuming their dataset meets the submission criteria for an aggregation site). A scientist also has the option of using the CDAT suite of libraries, developed by the \cite{WilliamsEtAl13}, GrADS, \cite{src:grads},and Ferret, \cite{src:HankinEtAl96}. The major critique with all of these though is that they mostly render the data as is, with limited support for statistical analysis and the application of typical meterological analysis. Support for even common machine learning techniques often requires extending the library; therefore for the purpose of this discussion it is somewhat useful to treat these tools more as plotting libraries than visualization systems. %%somewhere scope out/differentiate between plotting and viz


\section{Conclusion}
\label{sec:conclusion}


\pagebreak
\bibliographystyle{abbrv}
\bibliography{exam2}

\end{document}
